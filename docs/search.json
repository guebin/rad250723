[
  {
    "objectID": "exercise.html",
    "href": "exercise.html",
    "title": "",
    "section": "",
    "text": "pip install autogluon --extra-index-url https://download.pytorch.org/whl/cpu\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom autogluon.tabular import TabularPredictor\n\n/home/cgb2/anaconda3/envs/yechan/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n!wget https://raw.githubusercontent.com/guebin/rad250723/main/titanic.csv\n\n\ndf = pd.read_csv(\"titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n(1) df의 차원을 확인하는 코드를 작성하세요.\n(2) df의 처음 5개의 행을 출력하는 코드를 작성하세요.\n(3) df의 마지막 5개의 행을 출력하는 코드를 작성하세요.\n(4) df의 열을 출력하는 코드를 작성하세요.\n(5) train_test_split 함수를 이용하여 훈련자료와 테스트자료를 5:5로 나누는 코드를 작성하세요.\n\n단, 이때 테스트자료에서는 변수 “Survived”가 제외되어 있어야함.\n훈련자료는 df_train으로 테스트자료는 df_test로 저장되어 있을 것.\n\n(6) df_test의 처음 5개의 행을 출력해보세요. 승객의 생존유무를 나름의 논리로 추측해보세요. 추측의 결과가 맞는지 확인하는 코드를 제시하세요.\nhint: df에서 df_test의 인덱스에 해당하는 정보만 출력 (강의노트 p24 참고)\n(7) df_train의 성별 생존률을 확인하는 코드를 작성하세요.\n(8) “남성이면 사망하고, 여성이면 생존한다”라는 단순한 룰을 적용하여 df_test의 Survived열을 예측한다면 정확도는 얼마나 될까요?\n(9) df_train에서 10세미만승객의 생존률과 10세이상승객의 생존률을 구하는 코드를 작성하세요.\n(10) df_train에서 성별(Sex)과 탑승항구(Embarked)에 따른 생존률을 비교하는 코드를 작성하세요. 즉 아래의 표를 채우세요.\n\n\n\nSex\nEmbarked\nRate\n\n\n\n\nfemale\nC\n.\n\n\nfemale\nQ\n.\n\n\nfemale\nS\n.\n\n\nmale\nC\n.\n\n\nmale\nQ\n.\n\n\nmale\nS\n.\n\n\n\n(11) df_train에서 Sex, Pclass 열을 추출하고 pd.get_dummies 함수를 이용하여 원핫인코딩하세요.\n(12) 설명변수로 Sex, Age를 선택한 뒤 아래의 과정에 따라 의사결정나무모델을 훈련하고 성능을 확인하세요.\n\n데이터전처리: X, X_test 생성\n모형선택: DecisionTreeClassifier를 이용하여 의사결정나무 모델을 만들고 model변수에 저장\n적합: model의 .fit을 이용하여 훈련자료를 적합\n예측: model의 .predict를 이용하여 테스트자료를 예측하고 성능확인\n\n(13) 설명변수로 Sex, Age, Fare 를 선택한 뒤 아래의 과정에 따라 의사결정나무모델을 훈련하고 성능을 확인하세요.\n\n데이터전처리: X, X_test 생성\n모형선택: DecisionTreeClassifier를 이용하여 의사결정나무 모델을 만들고 model변수에 저장\n적합: model의 .fit을 이용하여 훈련자료를 적합\n예측: model의 .predict를 이용하여 테스트자료를 예측하고 성능확인\n\n(14) 설명변수로 “Pclass”, “Sex”, “Age”, “SibSp”, “Parch”, “Fare”, “Embarked”를 선택한 뒤 Autogluon을 이용하여 다양한 모델을 적합하고 성능을 확인하세요."
  },
  {
    "objectID": "qslides.html#imports",
    "href": "qslides.html#imports",
    "title": "AI 기반 데이터 사이언스",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom autogluon.tabular import TabularPredictor\n\n\n\n&lt;function IPython.core.formatters.PlainTextFormatter._type_printers_default.&lt;locals&gt;.&lt;lambda&gt;(obj, p, cycle)&gt;"
  },
  {
    "objectID": "qslides.html#titanic",
    "href": "qslides.html#titanic",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic",
    "text": "Titanic\nThe Titanic was a large passenger ship that departed from England in 1912. During its maiden voyage, the ship tragically collided with an iceberg and sank, resulting in the deaths of approximately 1,500 people. The film Titanic, directed by James Cameron in 1997, is a romance-disaster movie that tells a fictional love story set against the backdrop of this historical event. The movie portrays the grandeur of the ship, the social divide between the upper and lower classes, and the emotional turmoil of the characters as the disaster unfolds. It blends historical facts with imaginative storytelling to depict both the tragedy of the sinking and the intimate, human experiences that could have occurred during that fateful voyage.\n\nFigure: Poster image from Titanic (1997), depicting the iconic scene of Jack and Rose at the ship’s bow."
  },
  {
    "objectID": "qslides.html#the-story-of-titanic",
    "href": "qslides.html#the-story-of-titanic",
    "title": "AI 기반 데이터 사이언스",
    "section": "The Story of Titanic",
    "text": "The Story of Titanic\nChapter 1. Jack and Rose Board the Ship\nThe movie Titanic starts with two people from very different lives. Jack Dawson gets a Titanic ticket by chance when he plays a card game with his friend on the day the ship leaves. He runs to the dock and gets on the ship just before it leaves. His ticket is for third class, which is the cheapest.\nhttps://www.youtube.com/watch?v=tEM0I3ltp7M\nRose DeWitt Bukater is already on the ship at the beginning of the movie. She gets on the Titanic with her boyfriend Cal and her mother. They are rich and get special treatment. Rose wears fancy clothes, eats expensive food, and stays in a beautiful room. But she feels trapped and unhappy.\nhttps://www.youtube.com/watch?v=3uCi1g_aV-g"
  },
  {
    "objectID": "qslides.html#the-story-of-titanic-1",
    "href": "qslides.html#the-story-of-titanic-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "The Story of Titanic",
    "text": "The Story of Titanic\nChapter 2. They Fall in Love\nJack and Rose meet on the ship. Jack saves Rose when she is in danger. They talk, laugh, and spend time together. Soon, they fall in love.\nhttps://www.youtube.com/watch?v=erAQ9LkftwA\nhttps://www.youtube.com/watch?v=oklPl95DC8c"
  },
  {
    "objectID": "qslides.html#the-story-of-titanic-2",
    "href": "qslides.html#the-story-of-titanic-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "The Story of Titanic",
    "text": "The Story of Titanic\nChapter 3. The Sinking and the Divided Paths\nWhen the Titanic strikes the iceberg, chaos begins to unfold. But this chaos is not delivered equally to everyone on board.\nIn the film, a clear difference is shown between first-class and third-class passengers. First-class passengers are quickly and politely informed by crew members. They receive calm explanations, are given life jackets, and are carefully guided toward the lifeboats. They are seen dressing in formal clothes and preparing for evacuation in an orderly manner.\nMeanwhile, third-class passengers go a long time without any information about the accident. Many find the stairways and corridors blocked, unable to reach the upper decks. Families wander the ship in confusion, and others wait helplessly behind locked gates— a powerful image of the class-based gap in access to survival.\nAmid the growing panic, Rose’s fiancé, Cal, begins to search for her. As a wealthy gentleman, he urges Rose to get on a lifeboat and escape with him.\nhttps://www.youtube.com/watch?v=Gmw1q0CprEA"
  },
  {
    "objectID": "qslides.html#the-story-of-titanic-3",
    "href": "qslides.html#the-story-of-titanic-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "The Story of Titanic",
    "text": "The Story of Titanic\nChapter 4. The Lifeboat Scene\nCal tells Rose to get on a lifeboat. She agrees at first and is lowered away from Jack.\nBut as the boat goes down, Rose looks up at him. She suddenly jumps back onto the ship. She chooses to stay with Jack, even if it means risking her life.\nJack and Rose run through the sinking ship together. They try to find a way out as water floods the halls. Their love becomes stronger in the face of fear.\nhttps://www.youtube.com/watch?v=_qTZRD1_ybQ"
  },
  {
    "objectID": "qslides.html#the-story-of-titanic-4",
    "href": "qslides.html#the-story-of-titanic-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "The Story of Titanic",
    "text": "The Story of Titanic\nChapter 5. The End and the Memory\nAs the Titanic sinks deeper into the sea, Jack and Rose struggle to survive together until the very end. They cling to a broken piece of wood, floating in the freezing ocean.\nJack protects Rose and says, “Never give up. Never let go.” Rose holds his promise deep in her heart.\nWhen a lifeboat finally arrives, Rose is rescued. But Jack quietly slips into the cold water and disappears.\nLater, on the lifeboat, a rescuer asks for her name. She answers, “Rose Dawson.”\nhttps://www.youtube.com/watch?v=D7_SqyWiPOg"
  },
  {
    "objectID": "qslides.html#titanic-dataset",
    "href": "qslides.html#titanic-dataset",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\nThe Titanic dataset is a standard, real-world dataset that contains information about the passengers aboard the RMS Titanic, which sank in 1912. It is widely used as a classic resource for practicing machine learning, statistical analysis, and data visualization.\nThe Titanic dataset is commonly provided as a CSV file named titanic.csv, and can be obtained from various sources such as Kaggle, the seaborn package, or scikit-learn. We can load this data using the pandas library in Python."
  },
  {
    "objectID": "qslides.html#titanic-dataset-1",
    "href": "qslides.html#titanic-dataset-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\nThe Titanic data can be loaded as follows:\n\ndf = pd.read_csv(\"titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns"
  },
  {
    "objectID": "qslides.html#titanic-dataset-2",
    "href": "qslides.html#titanic-dataset-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\nIn this code, pd.read_csv(\"titanic.csv\") means reading the file titanic.csv from the current working directory, and storing its contents into a variable named df as a DataFrame object.\n\npd is a commonly used alias for the pandas package, which is typically imported using import pandas as pd.\nread_csv() is a function that reads data from a CSV (Comma-Separated Values) file and converts it into a pandas DataFrame, a tabular data structure.\n\nThis allows users to import structured data in table format and perform various kinds of analysis on it."
  },
  {
    "objectID": "qslides.html#titanic-dataset-3",
    "href": "qslides.html#titanic-dataset-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\nWhen working with data in pandas,the very first thing to check after loading a dataset is its overall structure. The most basic command used for this purpose is:\n\ndf.shape\n\n(891, 12)\n\n\nRunning this code returns the output (891, 12), which indicates that the DataFrame contains 891 rows and 12 columns. In this context, each row represents a single observation — in this case, an individual passenger on the Titanic. Each column represents a variable that describes a certain feature of the passenger, such as Age, Sex, Pclass, or Survived.\nTo explicitly show what variables are included in the dataset, it is helpful to inspect the column names directly. This can be done using the following command:\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"
  },
  {
    "objectID": "qslides.html#titanic-dataset-4",
    "href": "qslides.html#titanic-dataset-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\n\n\n\n\n\n\nTip\n\n\nUseful pandas Commands for Exploring Data Structure\nUsing commands like df.shape or df.columns to explore the structure of a dataset is a fundamental first step in any data analysis workflow. These commands help you understand what the data looks like, how many variables it contains, and how it’s organized. Below are some of the most useful pandas functions for this purpose:\n\ndf.shape: Returns the overall dimensions of the DataFrame as a tuple (rows, columns). This helps you quickly understand how many observations (rows) and variables (columns) the dataset contains.\ndf.columns: Displays a list of all column names in the DataFrame. This allows you to explicitly check what variables are included.\ndf.info(): Provides a concise summary of the DataFrame, including data types of each column, the number of non-null entries, and memory usage. It is especially useful for detecting missing values and distinguishing between numeric and categorical variables.\ndf.head() / df.tail(): Shows the first (or last) few rows of the dataset. This gives you a quick preview of actual values, formatting, and data units, making it easier to get a sense of how the data is structured.\ndf.describe(): Generates summary statistics for numeric columns, including mean, standard deviation, minimum, maximum, and quartiles. It helps in identifying variable scales, distributions, and potential outliers early in the analysis."
  },
  {
    "objectID": "qslides.html#titanic-dataset-5",
    "href": "qslides.html#titanic-dataset-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Dataset",
    "text": "Titanic Dataset\nUnderstanding this basic structure is an essential first step in any data analysis process. If there are too few rows, statistical results may not be reliable. On the other hand, if there are too many columns or if the data contains many missing values, preprocessing and feature selection may become more complex.\nFrom this output, we can see that the Titanic dataset includes 891 passengers and 12 variables, making it well-suited for analysis and practice in tasks such as classification, exploration, and visualization.\n\n\n\n\n\n\nNote\n\n\nConventional meaning of rows and columns\n\nRow: Represents a single observation or instance. For example, in the Titanic dataset, each row corresponds to one passenger.\nColumn: Represents a variable or feature that describes a specific aspect of each observation. For example, Age, Sex, and Survived are variables that describe the characteristics of each passenger."
  },
  {
    "objectID": "qslides.html#variables-in-the-titanic-dataset",
    "href": "qslides.html#variables-in-the-titanic-dataset",
    "title": "AI 기반 데이터 사이언스",
    "section": "Variables in the Titanic Dataset",
    "text": "Variables in the Titanic Dataset\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPassengerId\nUnique ID for each passenger.\n\n\nSurvived\nSurvival status (0 = No, 1 = Yes).\n\n\nPclass\nTicket class (1 = 1st, 2 = 2nd, 3 = 3rd).\n\n\nName\nFull name, includes title.\n\n\nSex\nGender (male or female).\n\n\nAge\nAge in years (may have missing values).\n\n\nSibSp\nNumber of siblings or spouses aboard.\n\n\nParch\nNumber of parents or children aboard.\n\n\nTicket\nTicket number.\n\n\nFare\nTicket fare (in pounds).\n\n\nCabin\nCabin number (many missing).\n\n\nEmbarked\nEmbarked shows boarding port: C (Cherbourg), Q (Queenstown), S (Southampton)."
  },
  {
    "objectID": "qslides.html#sample-observations-from-the-titanic-dataset",
    "href": "qslides.html#sample-observations-from-the-titanic-dataset",
    "title": "AI 기반 데이터 사이언스",
    "section": "Sample Observations from the Titanic Dataset",
    "text": "Sample Observations from the Titanic Dataset\nAfter loading the data, it is a good practice to print a few rows to understand how the dataset is structured.\n\ndf[:3]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n\n\n\n\n\nUsing the command df[:3], we can view the first three observations in the Titanic DataFrame.\nFor example, the first passenger is a 22-year-old male in 3rd class who boarded at Southampton and did not survive. The second passenger is a 38-year-old female in 1st class with cabin C85. She boarded at Cherbourg and survived. The third passenger is a 26-year-old female in 3rd class who also boarded at Southampton and survived."
  },
  {
    "objectID": "qslides.html#supervised-vs.-unsupervised-choosing-a-path",
    "href": "qslides.html#supervised-vs.-unsupervised-choosing-a-path",
    "title": "AI 기반 데이터 사이언스",
    "section": "Supervised vs. Unsupervised: Choosing a Path",
    "text": "Supervised vs. Unsupervised: Choosing a Path\nNow that we’ve reviewed a few sample observations and understood the structure of the dataset, it’s time to consider what we can actually do with this data.\nThe Titanic dataset allows for various types of analysis. For example, one could explore relationships between variables and discover hidden patterns using unsupervised learning. Alternatively, since the dataset includes a clear outcome variable (Survived), we can apply supervised learning to build predictive models.\nIn today’s lecture, we will focus on supervised learning. Our goal is to use features like gender, age, passenger class, and port of embarkation to predict whether a passenger survived the Titanic disaster."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set",
    "href": "qslides.html#training-set-vs-test-set",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nBefore building any predictive model, we first need to split our dataset into two parts: a training set and a test set.\n\nThe training set (df_train) is used to train the model. It learns patterns from this data, including how certain features relate to survival.\nThe test set (df_test) is used to evaluate how well the model performs on new, unseen data. This helps us assess generalization, not just memorization.\n\nBy separating our data this way, we can simulate how our model would behave in real-world scenarios — predicting survival outcomes for passengers it hasn’t seen before. In practice, we can use the train_test_split() function from sklearn."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-1",
    "href": "qslides.html#training-set-vs-test-set-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nIn our case, we used train_test_split(df, test_size=0.3, random_state=42) to split the Titanic dataset. This means that 70% of the data (712 passengers) is used for training, while the remaining 30% (179 passengers) is reserved for testing.\n\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_test = df_test.drop([\"Survived\"],axis=1)\n\nAfter splitting, we removed the Survived column from the test set using df_test = df_test.drop([\"Survived\"], axis=1) to simulate real-world prediction, where the correct answer is unknown.\n\n\n\n\n\n\n\nCode Explanation\n\n\nHere’s a quick explanation of the main options used.\n\ntest_size=0.2: 20% of the data goes into the test set, and 80% into the training set.\nrandom_state=42: Ensures that the random split is reproducible every time.\n.drop([\"Survived\"], axis=1): Removes the Survived column from df_test.\naxis=1 means “drop a column” (not a row)."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-2",
    "href": "qslides.html#training-set-vs-test-set-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nTo confirm that the data has been properly split, we can check the shape of each DataFrame using the following command:\n\ndf.shape, df_train.shape, df_test.shape\n\n((891, 12), (712, 12), (179, 11))\n\n\nThis result can be interpreted as follows:\n\n(891, 12): The full DataFrame df contains 891 observations (rows) and 12 variables (columns).\n(712, 12): After using train_test_split(), 80% of the data — 712 rows — was assigned to the training set df_train, which still contains all 12 columns.\n(179, 11): The test set df_test contains the remaining 20% — 179 rows — but since we explicitly dropped the Survived column, the number of columns is reduced to 11."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-3",
    "href": "qslides.html#training-set-vs-test-set-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nAfter splitting the data using train_test_split(), we can check the first few rows of each subset to verify how the data was divided.\nFor instance, when we look at df_train[:2], we can see that the first two rows of the training set correspond to rows with original indices 331 and 733 from the full dataset df.\n\ndf_train[:2]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n331\n332\n0\n1\nPartner, Mr. Austen\nmale\n45.5\n0\n0\n113043\n28.5\nC124\nS\n\n\n733\n734\n0\n2\nBerriman, Mr. William John\nmale\n23.0\n0\n0\n28425\n13.0\nNaN\nS\n\n\n\n\n\n\n\nSimilarly, df_test[:2] shows rows from the original dataset with indices 709 and 439.\n\ndf_test[:2]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n\n\n\n\n\nNote that the Survived column has been removed."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-4",
    "href": "qslides.html#training-set-vs-test-set-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\n\nFigure: This illustration visually explains the structure of df_train and df_test for easier understanding. The training set (df_train) includes the Survived information, while the test set (df_test) does not. This image was generated using Perplexity."
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-5",
    "href": "qslides.html#training-set-vs-test-set-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nWe assume that the observed data consists of df_train and df_test. We use df_train for training. The goal of training is to correctly predict the survival status when given new data like df_test. If we have studied df_train carefully, we should be able to guess whether the following two passengers survived or not.\n\ndf_test[:5]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n840\n841\n3\nAlhomaki, Mr. Ilmari Rudolf\nmale\n20.0\n0\n0\nSOTON/O2 3101287\n7.9250\nNaN\nS\n\n\n720\n721\n2\nHarper, Miss. Annie Jessie \"Nina\"\nfemale\n6.0\n0\n1\n248727\n33.0000\nNaN\nS\n\n\n39\n40\n3\nNicola-Yarred, Miss. Jamila\nfemale\n14.0\n1\n0\n2651\n11.2417\nNaN\nC"
  },
  {
    "objectID": "qslides.html#training-set-vs-test-set-6",
    "href": "qslides.html#training-set-vs-test-set-6",
    "title": "AI 기반 데이터 사이언스",
    "section": "Training Set vs Test Set",
    "text": "Training Set vs Test Set\nAnswer revealed… (Though in practice, we wouldn’t actually know the answer)\n\ndf.iloc[df_test[:5].index]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n1\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n0\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n840\n841\n0\n3\nAlhomaki, Mr. Ilmari Rudolf\nmale\n20.0\n0\n0\nSOTON/O2 3101287\n7.9250\nNaN\nS\n\n\n720\n721\n1\n2\nHarper, Miss. Annie Jessie \"Nina\"\nfemale\n6.0\n0\n1\n248727\n33.0000\nNaN\nS\n\n\n39\n40\n1\n3\nNicola-Yarred, Miss. Jamila\nfemale\n14.0\n1\n0\n2651\n11.2417\nNaN\nC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\nThe code df.iloc[df_test[:2].index] retrieves the original rows (including the Survived column) for the first two passengers in df_test.\n\ndf_test[:2] selects the first two rows from the test set.\n.index extracts their original row positions from the full DataFrame df.\ndf.iloc[...] uses these positions to return the corresponding rows from df, including the true labels.\n\n\n\n\n\nLet’s play a game: try to guess who survived the Titanic!"
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nDo you remember that iconic scene from Titanic—when Rose boards the lifeboat? Rose is seated in the lifeboat, looking up at Jack and Cal who are still on the ship’s deck. Jack stands silently, watching her leave, while other passengers around them reflect the chaos of the moment. Though no words are spoken, their eyes are locked, full of emotion.\n\nFigure: Rose is leaving the ship aboard a lifeboat, while Jack and Cal watch her from the deck."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-1",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nFrom her place in the lifeboat, Rose gazes up at the deck. Her expression is filled with uncertainty and longing as she looks toward Jack. The moment captures the emotional weight of her decision. Soon after, driven by love and instinct, Rose jumps out of the lifeboat and runs back to the sinking ship—a choice that defines one of the most iconic scenes in the film.\n\nFigure: Rose looks up at Jack from the lifeboat, feeling sad and not wanting to say goodbye."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-2",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nThese scenes may be emotionally powerful, but let’s set aside the emotion for a moment and look at them objectively.\nWho is on the lifeboat? We see Rose and several other women being rescued.\nWho remains on the deck? Jack and other men are watching the lifeboats leave from the sinking ship.\nThis contrast raises an important question:\n\nWas gender a factor in determining who had a better chance of survival?\n\nLet’s turn to the data to explore this further."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-3",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nTo analyze the impact of gender on survival outcomes, we use the following code to compute survival rates by sex.\n\ndf_train.groupby(\"Sex\")['Survived']\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7a6e8017b350&gt;\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\nThis command does the following:\n\ngroupby(\"Sex\"): groups the passengers by their gender (male or female)\n['Survived'].mean(): calculates the average survival rate for each group (since 1 = survived, 0 = did not survive, the mean gives the survival proportion)\n.reset_index(): turns the grouped result back into a clean DataFrame\n\nThe result shows how survival rates differ dramatically between males and females. Let’s look at the numbers."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-4",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nThis output shows the average survival rate by gender.\n\nFor females, the survival rate is approximately 73.9%.\nFor males, it’s about 18.6%.\n\nIn other words, just knowing the passenger’s gender already gives us a strong signal about their likelihood of survival. A variable like this, with a clear split in outcomes, can be very useful in building predictive models. Based on this result, we can try a simple rule-based prediction: Predict that all females survived (1), and all males did not survive (0). Let’s test how accurate this strategy actually is."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-5",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nIn this analysis, the goal is to predict survival status. The variable we are trying to predict is called the response variable (or dependent variable), which in this case is:\n\ny_test = df.iloc[df_test.index]['Survived']\n\nThis gives us a vector of length 179 containing actual survival outcomes from the test set: 0 means the passenger did not survive, and 1 means they did. On the other hand, the variable we use to make predictions is called the explanatory variable (or independent variable). Here, we use the Sex column and apply a simple rule:\n\nyhat_test = (df_test['Sex']  == \"female\")\n\nThis creates a Boolean vector that predicts survival based on whether the passenger is female. So in summary:\n\ny_test is the response variable — the true survival outcomes.\nyhat_test is based on an explanatory variable — a simple prediction using gender."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-6",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-6",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nBy comparing yhat_test and y_test using (yhat_test == y_test).mean(), we calculate the accuracy of our prediction, which tells us how informative the Sex variable is for predicting survival.\n\n(yhat_test == y_test).mean()\n\n0.782123\n\n\nThe result shows an accuracy of approximately 78.2%, which is a significant improvement over random guessing (which would yield about 50% accuracy). This demonstrates that even a simple rule based solely on the “Sex” variable can produce surprisingly strong predictions — all without using any machine learning."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-7",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-7",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nLet’s go back to the movie for a moment. Was it really just\n\n“women first”?\n\nIn situations like this, we often expect the most vulnerable — women and children — to be given priority. Even the film hints at this principle. Now let’s take a closer look at the data to see whether younger passengers were also more likely to survive.\nhttps://www.youtube.com/watch?v=W0kURU_2H3c"
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-8",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-8",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nThe overall survival rate among all passengers was only about 37.6%, meaning that less than half of the people on board survived the disaster.\n\ndf_train['Survived'].mean()\n\n0.376404\n\n\nBut what about children?\n\ndf_train.query(\"Age &lt;10\")['Survived'].mean()\n\n0.603774\n\n\nWhen we look specifically at passengers under the age of 10, their survival rate rises significantly to about 60.4%. This suggests that younger passengers were indeed given some level of priority during evacuation, supporting the idea portrayed in the film — that the principle of “women and children first” may have been reflected in real-life decisions during the sinking."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-9",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-9",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\n\n\n\n\n\n\n\nCode Explanation\n\n\nThe following code filters the DataFrame to include only rows where the Age column is less than 10.\ndf_train.query(\"Age &lt; 10\")\nThe query() method allows you to write filtering conditions as strings, making the code clean and easy to read.\nFor example:\n\ndf.query(\"Fare &gt; 100\") → selects passengers who paid more than 100\ndf.query(\"Sex == 'female'\") → selects female passengers\n\nYou can also combine multiple conditions:\ndf.query(\"Pclass == 1 and Sex == 'female'\")\nquery() is a convenient and readable way to filter data based on conditions."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-10",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-10",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nThe plot below visualizes the age distribution of passengers based on their survival status. The blue area shows the age distribution of those who survived, while the red area represents those who did not. Notably, children under the age of 10 show a relatively higher survival rate, supporting the idea that the “women and children first” principle may have been applied during the evacuation.\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# 공통 X축 범위 설정\nx_range = (0, 80)\n\n# KDE plot for survived == 0 and 1\nsns.kdeplot(data=df_train[df_train.Survived == 0], x=\"Age\", fill=True, label=\"Did Not Survive\", color=\"salmon\")\nsns.kdeplot(data=df_train[df_train.Survived == 1], x=\"Age\", fill=True, label=\"Survived\", color=\"skyblue\")\n\nax.set_xlim(x_range)\nax.set_title(\"Age Distribution by Survival Status\")\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Density\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\nFigure: Survival Rate by Age Distribution"
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-11",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-11",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nNow let’s try a new prediction strategy:\n\nIf the passenger is female, predict that they survived.\nIf the passenger is under 10 years old, also predict that they survived.\n\n\ny_test = df.iloc[df_test.index]['Survived']\nyhat_test = (df_test['Sex']  == \"female\") | (df_test['Age'] &lt; 10)\n\nNow let’s compare y_test and yhat_test to evaluate the prediction accuracy:\n\n(y_test == yhat_test).mean()\n\n0.787709\n\n\nThis strategy yields an accuracy of 0.787709, or about 78.8%. That’s a slight improvement over using only the sex variable, which gave an accuracy of 78.2%."
  },
  {
    "objectID": "qslides.html#titanic-predictor-no-ml-just-guess-12",
    "href": "qslides.html#titanic-predictor-no-ml-just-guess-12",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic Predictor: No ML, Just Guess",
    "text": "Titanic Predictor: No ML, Just Guess\nThe improvement isn’t as dramatic as one might expect. Why is that?\nIt turns out that there aren’t many passengers under the age of 10 in the test set. The only time our prediction rule changes (compared to using gender alone) is when a passenger is male and under 10 — but such cases are rare. If we check the data:\n\ndf_test.query(\"Sex == 'male' and Age &lt; 10\")\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n63\n64\n3\nSkoog, Master. Harald\nmale\n4.00\n3\n2\n347088\n27.900\nNaN\nS\n\n\n165\n166\n3\nGoldsmith, Master. Frank John William \"Frankie\"\nmale\n9.00\n0\n2\n363291\n20.525\nNaN\nS\n\n\n78\n79\n2\nCaldwell, Master. Alden Gates\nmale\n0.83\n0\n2\n248738\n29.000\nNaN\nS\n\n\n\n\n\n\n\nWe find that there are only 3 such passengers in the test set. That’s why the gain is modest. It’s a bit disappointing, but even so, we can be reasonably satisfied with the small improvement that age provides."
  },
  {
    "objectID": "qslides.html#generalization",
    "href": "qslides.html#generalization",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nSo far, we’ve seen that gender and age played an important role in survival. But are those the only factors that mattered?\nThink back to that unforgettable scene from the film: First-class passengers are calmly escorted to the lifeboats by the crew, while third-class passengers remain behind locked gates, confused and unable to escape. This isn’t just cinematic drama — it raises an important question:\n\nDid survival also depend on ticket class or fare price?\n\nLet’s explore the data further to find out."
  },
  {
    "objectID": "qslides.html#generalization-1",
    "href": "qslides.html#generalization-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nThis code calculates the average survival rate by passenger class (Pclass):\n\ndf_train.groupby(\"Pclass\")['Survived'].mean().reset_index()\n\n\n\n\n\n\n\n\nPclass\nSurvived\n\n\n\n\n0\n1\n0.607362\n\n\n1\n2\n0.483444\n\n\n2\n3\n0.241206\n\n\n\n\n\n\n\nThe output shows the following survival rates:\n\n1st class (Pclass = 1): approximately 60.7%\n2nd class (Pclass = 2): approximately 48.3%\n3rd class (Pclass = 3): approximately 24.1%\n\nIn other words, passengers in higher classes had a much better chance of survival. This aligns with scenes from the movie, where first-class passengers were often prioritized during evacuation. The data suggests that passenger class (Pclass) is a strong explanatory variable when predicting survival on the Titanic."
  },
  {
    "objectID": "qslides.html#generalization-2",
    "href": "qslides.html#generalization-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nThe prediction based solely on passenger class (predicting survival if Pclass == 1) achieves an accuracy of approximately 70.4%.\n\ny_test = df.iloc[df_test.index]['Survived']\nyhat_test = (df_test['Pclass'] == 1) \n(yhat_test == y_test).mean()\n\n0.703911\n\n\nWhile this result is quite good, it falls short of the 78.2% accuracy obtained when predicting based solely on gender (predicting survival if Sex == “female”).\nThis suggests that gender is a stronger explanatory variable than passenger class when it comes to predicting survival on the Titanic. In other words, knowing someone’s gender gives us more predictive power than knowing their ticket class alone."
  },
  {
    "objectID": "qslides.html#generalization-3",
    "href": "qslides.html#generalization-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nCould we consider both gender and passenger class together? Doing so may reveal more detailed and accurate survival patterns.\n\ndf_train.groupby([\"Sex\",\"Pclass\"])[\"Survived\"].mean().reset_index()\n\n\n\n\n\n\n\n\nSex\nPclass\nSurvived\n\n\n\n\n0\nfemale\n1\n0.957143\n\n\n1\nfemale\n2\n0.966667\n\n\n2\nfemale\n3\n0.486957\n\n\n3\nmale\n1\n0.344086\n\n\n4\nmale\n2\n0.164835\n\n\n5\nmale\n3\n0.141343\n\n\n\n\n\n\n\nFor example, female passengers in 1st class had a survival rate of about 95.7%, while male passengers in 3rd class had a survival rate of only 14.1%. These stark contrasts show that the combination of gender and class provides a much stronger signal than either variable alone. In short, considering both Sex and Pclass together gives us a more powerful strategy for predicting survival."
  },
  {
    "objectID": "qslides.html#generalization-4",
    "href": "qslides.html#generalization-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nSo, let’s design a prediction rule based on the survival rates we just observed. We saw that:\n\nWomen in 1st and 2nd class had very high survival rates.\nMen in 2nd and 3rd class had very low survival rates.\nThe more ambiguous cases were female passengers in 3rd class and male passengers in 1st class, but even those had survival rates below 50%.\n\nGiven this, a reasonable strategy would be:\n\nPredict survived only if the passenger is female and in 1st or 2nd class. Otherwise, predict not survived."
  },
  {
    "objectID": "qslides.html#generalization-5",
    "href": "qslides.html#generalization-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nHere’s how we can implement this in code:\n\nyhat_test = ((df_test['Sex']  == \"female\") & (df_test['Pclass'] &lt;3))\n(y_test==yhat_test).mean()\n\n0.765363\n\n\nBut wait — the result is unexpected! The accuracy turns out to be 0.765, which is lower than the simpler rule of just predicting that all women survived, which gave an accuracy of 0.782.\nThat’s surprising! Despite using more detailed information (both gender and class), the performance actually drops. It turns out that this added complexity doesn’t always translate to better predictions — and sometimes, simpler is better."
  },
  {
    "objectID": "qslides.html#generalization-6",
    "href": "qslides.html#generalization-6",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nThis result contradicts what we saw earlier.\nPreviously, when we combined Sex and Age in our prediction rule, the accuracy improved — even if only slightly. That experience reinforced a common belief in data modeling:\n\n“Adding more explanatory variables will naturally improve model performance.”\n\nBut now we’re seeing the opposite — adding another meaningful variable (Pclass) actually decreased our accuracy. How can this be?"
  },
  {
    "objectID": "qslides.html#generalization-7",
    "href": "qslides.html#generalization-7",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nCould it be that we made a mistake somewhere? Let’s take a step back and carefully consider how we’ve been approaching the problem:\n\nWe learn a rule from the training set.\nThen we apply that rule to the test set.\n\nBut here’s something to think about:\n\nWhat if the patterns we discovered in the training data don’t hold in the test data?\n\nThat’s a real possibility — and it’s a fundamental challenge in any kind of predictive modeling. If a rule seems effective in training but doesn’t generalize well, then applying it to unseen data may lead to worse performance, not better."
  },
  {
    "objectID": "qslides.html#generalization-8",
    "href": "qslides.html#generalization-8",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nLet’s revisit the training environment — specifically, the df_train dataset. Previously, we applied a simple rule based only on gender, which gave us the following result:\n\ny = df_train['Survived']\nyhat = (df_train['Sex']  == \"female\") \n(y == yhat ).mean()\n\n0.787921\n\n\nThis gave an accuracy of 0.787921. Now, what happens when we consider both gender and class?\n\nyhat = ((df_train['Sex']  == \"female\") & (df_train['Pclass'] &lt;3))\n(y == yhat ).mean()\n\n0.792135\n\n\nThis gives a slightly higher accuracy of 0.792135. So in the training set, it was better to predict that only 1st- and 2nd-class women survived, rather than all women."
  },
  {
    "objectID": "qslides.html#generalization-9",
    "href": "qslides.html#generalization-9",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nThe key difference between these two strategies lies in how we handle 3rd-class female passengers:\n\nThe older rule predicts they survived.\nThe newer rule predicts they did not survive.\n\nAnd the justification for the new rule comes from this survival rate:\n\ndf_train.query(\"Sex == 'female' and Pclass ==3\")['Survived'].mean()\n\n0.486957\n\n\nThat result — 0.486957 — should make us a little uneasy. What if that number had been something like 0.499999? Can we confidently say that 3rd-class females were more likely to die than survive?"
  },
  {
    "objectID": "qslides.html#generalization-10",
    "href": "qslides.html#generalization-10",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nTo help us better understand the issue, let’s create a hypothetical DataFrame called df_test_full:\n\ndf_test_full = df.iloc[df_test.index]\ndf_test_full[:3]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n1\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n0\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n840\n841\n0\n3\nAlhomaki, Mr. Ilmari Rudolf\nmale\n20.0\n0\n0\nSOTON/O2 3101287\n7.9250\nNaN\nS\n\n\n\n\n\n\n\nThis dataset contains the same passengers as df_test, in the same order — but with one key difference: it also includes the actual Survived values for each passenger.\n\ndf_test[:3]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n840\n841\n3\nAlhomaki, Mr. Ilmari Rudolf\nmale\n20.0\n0\n0\nSOTON/O2 3101287\n7.9250\nNaN\nS"
  },
  {
    "objectID": "qslides.html#generalization-11",
    "href": "qslides.html#generalization-11",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nLet’s take a look at how the survival rates break down by gender and passenger class in the test set using df_test_full\n\ndf_test_full.groupby([\"Sex\",\"Pclass\"])[\"Survived\"].mean().reset_index()\n\n\n\n\n\n\n\n\nSex\nPclass\nSurvived\n\n\n\n\n0\nfemale\n1\n1.000000\n\n\n1\nfemale\n2\n0.750000\n\n\n2\nfemale\n3\n0.551724\n\n\n3\nmale\n1\n0.448276\n\n\n4\nmale\n2\n0.117647\n\n\n5\nmale\n3\n0.109375\n\n\n\n\n\n\n\nFocus especially on the survival rate of 3rd-class females. Does this test set result align with what we saw in the training set? Can we still be confident that the rule we learned — that only 1st- and 2nd-class women survived — generalizes well?"
  },
  {
    "objectID": "qslides.html#generalization-12",
    "href": "qslides.html#generalization-12",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nSometimes, a rule that works well on the training data fails to perform equally well on the test data. This situation reflects a failure to generalize — the idea that patterns found in one dataset (training) do not always hold in another (test).\nImproving generalization means ensuring that the rules we discover in training remain effective when applied to new, unseen data. One of the most straightforward ways to enhance generalization is to increase the amount of data. With more observations, we can estimate probabilities and patterns more accurately, leading to more reliable decision-making."
  },
  {
    "objectID": "qslides.html#generalization-13",
    "href": "qslides.html#generalization-13",
    "title": "AI 기반 데이터 사이언스",
    "section": "Generalization",
    "text": "Generalization\nIn addition, including too many variables or rules can actually reduce generalization performance. At first glance, adding more rules might seem helpful, but in practice, some rules may only appear useful due to random patterns in the training data — and fail to apply to other datasets.\nTo summarize, we can improve generalization by:\n\nIncreasing the number of observations, or\nReducing the number of rules or features used for prediction.\n\nSince collecting more data is often difficult in real-world settings, much of the research in modeling focuses on how to control model complexity and select only the most meaningful predictors."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml",
    "href": "qslides.html#titanic-from-intuition-to-ml",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nSo far, we’ve predicted Titanic survival outcomes using simple, rule-based intuition.\nWe started with rules like “predict survival for females and non-survival for males,” and then gradually incorporated variables like age and class to refine our predictions.\nHowever, as the number of variables increases and their relationships grow more complex, crafting reliable rules manually becomes extremely difficult.\n\nIt’s time to move beyond human intuition — and turn to machine learning.\n\nFrom here on, we’ll stop defining rules ourselves.\nInstead, we’ll use automated tools and machine learning algorithms to train models directly from the data.\nAs a first step, we’ll introduce a powerful yet intuitive model called the Decision Tree, which can discover complex patterns on its own — no human-designed rules required."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-1",
    "href": "qslides.html#titanic-from-intuition-to-ml-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nTo move beyond manual rule-making, we now turn to decision trees — a model that can automatically learn patterns from data. To maintain continuity with our previous approach, we will again use Sex and Pclass as our explanatory variables.\nHowever, since machine learning algorithms cannot directly handle text or categorical variables, we must first preprocess the data into a numeric format that the model can understand. For this, we use the get_dummies() function from the pandas library:\n\nX = pd.get_dummies(df_train[['Sex','Pclass']])\nX[:5]\n\n\n\n\n\n\n\n\nPclass\nSex_female\nSex_male\n\n\n\n\n331\n1\nFalse\nTrue\n\n\n733\n2\nFalse\nTrue\n\n\n382\n3\nFalse\nTrue\n\n\n704\n3\nFalse\nTrue\n\n\n813\n3\nTrue\nFalse"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-2",
    "href": "qslides.html#titanic-from-intuition-to-ml-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nThis output is the result of using pd.get_dummies(df_train[['Sex','Pclass']]), which transforms the selected features into a format suitable for machine learning models. Here’s what each column represents:\n\nPclass: Indicates the passenger’s ticket class. Lower numbers represent higher-class cabins (e.g., 1st class is better than 3rd class).\nSex_female: A boolean column where True means the passenger is female, and False otherwise. This is created through one-hot encoding of the Sex variable.\nSex_male: Similarly, True means the passenger is male. This is the complement of Sex_female.\n\nBy converting the categorical Sex column into binary features, the model can handle this data numerically. The Pclass column remains unchanged since it is already numeric."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-3",
    "href": "qslides.html#titanic-from-intuition-to-ml-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow let’s prepare the rest of the data for training and evaluation:\n\nX_test = pd.get_dummies(df_test[['Sex','Pclass']])\ny = df_train['Survived']\ny_test = df['Survived'][df_test.index]\n\nHere’s what each line does:\n\nX_test = pd.get_dummies(df_test[['Sex','Pclass']]): Performs one-hot encoding on the test set to match the format of the training data (X).\ny = df_train['Survived']: Sets the target variable (whether a passenger survived) for the training data.\ny_test = df['Survived'][df_test.index]: Retrieves the actual survival labels for the test set using their index from the original dataset. These values will be used to evaluate the model’s predictions."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-4",
    "href": "qslides.html#titanic-from-intuition-to-ml-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow that we’ve finished preparing the data, it’s time to choose a model. As mentioned earlier, we’ll start with a Decision Tree. But before we dive in, let’s take a moment to briefly introduce what a decision tree actually is.\nA Decision Tree is a machine learning model that works much like the game of 20 Questions. In that game, one person thinks of an object (like an animal or a job), and the other person tries to guess it by asking a series of yes/no questions such as: “Can it fly?”, “Does it have four legs?”, or “Is it a human?” Each question helps narrow down the possibilities, and with enough well-chosen questions, the answer eventually becomes clear.\nA decision tree follows the same idea. Given some data, it makes a prediction by asking a series of simple, binary questions. In the Titanic survival prediction task, for example, a decision tree might ask:\n\n“Is the passenger female?”\n“Is the ticket class 1st class?”\n“Is the passenger under 10 years old?”"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-5",
    "href": "qslides.html#titanic-from-intuition-to-ml-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nEach question divides the data into two branches — those that meet the condition and those that don’t. By following these branches step by step, the tree gradually narrows down the group of passengers and finally reaches a prediction, such as “Survived” or “Did not survive.” This final prediction is stored in what’s called a leaf node of the tree.\nWhat makes decision trees especially useful is their simplicity and interpretability. They don’t require complex math or feature transformations, and the decision path for each prediction can be traced back in plain language. This makes them easy to understand and explain — even to someone without a background in machine learning.\nIn summary, a decision tree is like a smart version of 20 Questions: it figures out what questions to ask based on the data, and uses the answers to make accurate predictions."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-6",
    "href": "qslides.html#titanic-from-intuition-to-ml-6",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow let’s take a look at how to use a decision tree in Python. The code below creates a decision tree model with default settings:\n\nmodel = DecisionTreeClassifier(random_state=0)\n\nThe DecisionTreeClassifier model object works primarily through two key methods: fit() and predict(). The .fit(X, y) method is used to train the model by learning patterns in the data. It takes the input features X and the corresponding target labels y, and builds a tree structure that splits the data based on conditions like “Is the passenger female?” or “Is the ticket class 1st class?” The goal is to find the best questions (or splits) that help classify the target variable as accurately as possible. Once the model is trained, the .predict(X_test) method can be used to make predictions on new data. For each row in X_test, the model follows the learned decision rules down the tree to arrive at a prediction, such as whether a passenger survived or not. In short, .fit() builds the decision rules from training data, and .predict() applies those rules to make predictions."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-7",
    "href": "qslides.html#titanic-from-intuition-to-ml-7",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nLet’s now take a look at the code below. It is used to train the model and make predictions on the test data:\n\nmodel.fit(X,y) \nyhat_test = model.predict(X_test)\n\nThe first line, model.fit(X, y), trains the DecisionTreeClassifier model using the training data. The second line, model.predict(X_test), uses the trained model to predict survival for each passenger in the test set. By running this code, we get the model’s predictions automatically based on the patterns it learned from the training data.\nLet’s now take a look at the accuracy of the survival predictions made by the Decision Tree model:\n\n(yhat_test == y_test).mean()\n\n0.765363"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-8",
    "href": "qslides.html#titanic-from-intuition-to-ml-8",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nThe result is 0.765363 — and interestingly, it’s exactly the same as the accuracy from this simple rule-based prediction:\n\nyhat_test = (df_test['Sex'] == 'female') & (df_test['Pclass'] &lt; 3)\n(y_test == yhat_test).mean()\n\n0.765363\n\n\nDisappointed? You shouldn’t be.\nIn fact, this result is reassuring: it shows that the Decision Tree isn’t performing some mysterious magic, but rather confirming patterns we already identified — such as\n\n“1st- and 2nd-class females were more likely to survive.”\n\nIn other words, instead of inventing new rules through complex computation, the model is making good use of meaningful patterns already present in the data."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-9",
    "href": "qslides.html#titanic-from-intuition-to-ml-9",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow, let’s take things a step further and consider all available features. We can inspect the column names using:\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\nThis gives us a list of variables such as “PassengerId”, “Pclass”, “Name”, “Sex”, “Age”, “SibSp”, “Parch”, “Ticket”, “Fare”, “Cabin”, and “Embarked”. Since “Survived” is the target variable we’re trying to predict, we’ll exclude it from the list of features used for training. So, our final list of features will be:\n\nfeatures = [\"PassengerId\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-10",
    "href": "qslides.html#titanic-from-intuition-to-ml-10",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow let’s include all these features in the model. Because some of them are categorical variables, we need to preprocess them using one-hot encoding. We can do this with the following code:\n\nX = pd.get_dummies(df_train[features])\nX_test = pd.get_dummies(df_test[features])\n\nLet’s now try training a model using all available features. But… an error occurs! Why did this happen?\nmodel = DecisionTreeClassifier()\nmodel.fit(X, y)\nyhat_test = model.predict(X_test)"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-11",
    "href": "qslides.html#titanic-from-intuition-to-ml-11",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nLet’s look into what caused the error. To debug this, we examine a small portion of the preprocessed data:\n\npd.get_dummies(df_train[2:4])\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Hansen, Mr. Henrik Juul\nName_Tikkanen, Mr. Juho\nSex_male\nTicket_350025\nTicket_STON/O 2. 3101293\nEmbarked_S\n\n\n\n\n382\n383\n0\n3\n32.0\n0\n0\n7.9250\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\n704\n705\n0\n3\n26.0\n1\n0\n7.8542\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nDid you catch the issue? Look closely — the Name column has been one-hot encoded into columns like:\n\nName_Hansen, Mr. Henrik Juul\nName_Tikkanen, Mr. Juho\n\nNow, here’s the problem: these exact names appear only in the training set. They don’t exist in the test set, so when we apply the same code to df_test, those columns are missing — which causes a mismatch in the number of features between training and test data."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-12",
    "href": "qslides.html#titanic-from-intuition-to-ml-12",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nLet’s look at the result of our preprocessing:\n\nX = pd.get_dummies(df_train[features])\nX_test = pd.get_dummies(df_test[features])\nlen(X.columns), len(X_test.columns)\n\n(1398, 401)\n\n\nSee the problem? The training set ended up with 1398 columns, while the test set has only 401 columns. This is because the get_dummies() function creates columns based on the unique values present in each dataset. For example, if a name or ticket appears only in the training set, get_dummies() will create a column for it — but that column will be missing in the test set, leading to a mismatch. As a result, the model cannot be trained and tested on data with different structures, which is why the error occurs. To fix this, we’ll need to ensure that both X and X_test have the same columns."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-13",
    "href": "qslides.html#titanic-from-intuition-to-ml-13",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow, we’ll select the features to use as follows:\n\nfeatures = [\"PassengerId\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n\nThe excluded features are Name, Ticket, and Cabin. These columns typically contain highly unique or overly specific values. If we apply get_dummies() to them, the resulting feature sets in the training and test data may end up with mismatched columns. This mismatch would cause errors during model training or prediction, so we exclude them.\nNow let’s apply one-hot encoding using the code below:\n\nX = pd.get_dummies(df_train[features])\nX_test = pd.get_dummies(df_test[features])\nlen(X.columns), len(X_test.columns)\n\n(11, 11)\n\n\nAs you can see, both datasets now have exactly 11 columns. With the data properly aligned, we’re finally ready to train the model!"
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-14",
    "href": "qslides.html#titanic-from-intuition-to-ml-14",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nNow it’s time to apply a machine learning model.\n\nmodel = DecisionTreeClassifier(random_state=0)\nmodel.fit(X, y)\nyhat_test = model.predict(X_test)\n(y_test == yhat_test).mean()\n\n0.731844\n\n\nThe result is in — the model yields an accuracy of 0.73. But the outcome feels disappointing. Despite considering all available variables, the model fails to outperform the following simple rule:\n\n(y_test == (df_test['Sex']==\"female\")).mean()\n\n0.782123\n\n\nJust predicting survival for all female passengers results in a higher accuracy."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-15",
    "href": "qslides.html#titanic-from-intuition-to-ml-15",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nWhy did this happen?\nIt reminds us of an earlier observation: adding more variables to the model doesn’t always lead to better performance, especially when we started from the “Sex” variable alone.\n\nfeatures\n\n['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n\nLet’s take a closer look at the current set of features.\nSome splits in the decision tree may be unnecessary and harm generalization. Is there any variable we can reasonably remove?\nOne variable that stands out is PassengerId. This is merely an identifier — it doesn’t carry any semantic meaning related to survival. Would it make sense to create a rule like: “If PassengerId &gt; 600, then the passenger survived”? That clearly makes no logical sense and would likely not generalize beyond this dataset.\nTherefore, it’s reasonable to remove the PassengerId feature before retraining the model."
  },
  {
    "objectID": "qslides.html#titanic-from-intuition-to-ml-16",
    "href": "qslides.html#titanic-from-intuition-to-ml-16",
    "title": "AI 기반 데이터 사이언스",
    "section": "Titanic: From Intuition to ML",
    "text": "Titanic: From Intuition to ML\nWe revised the feature list to exclude PassengerId and focused on more meaningful variables:\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX = pd.get_dummies(df_train[features])\nX_test = pd.get_dummies(df_test[features])\nmodel = DecisionTreeClassifier(random_state=0)\nmodel.fit(X, y)\nyhat_test = model.predict(X_test)\n(y_test == yhat_test).mean()\n\n0.804469\n\n\nThe result?\n\n0.804469!!\n\nFinally, we achieved a decent-performing model. Removing the irrelevant PassengerId feature appears to have improved generalization and accuracy."
  },
  {
    "objectID": "qslides.html#automl",
    "href": "qslides.html#automl",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nTraditional machine learning comes with several limitations:\n\nYou need to study and understand the model before using it.\nYou must carefully transform the data into a suitable format.\nHonestly, wouldn’t it be great if someone else just analyzed everything for us?\n\nLet’s take a look at AutoML."
  },
  {
    "objectID": "qslides.html#automl-1",
    "href": "qslides.html#automl-1",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nAutoML stands for Automated Machine Learning. It refers to the process of automating the end-to-end pipeline of building a machine learning model. This includes:\n\nPreprocessing the data\nSelecting and tuning models\nEvaluating performance\nMaking final predictions\n\nAutoML is designed to make machine learning accessible, efficient, and robust, even for those without deep expertise."
  },
  {
    "objectID": "qslides.html#automl-2",
    "href": "qslides.html#automl-2",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nThere are several well-known AutoML tools available today:\n\nGoogle AutoML (Cloud-based, integrates with Google Cloud services)\nAuto-sklearn (based on scikit-learn, focuses on pipelines)\nH2O AutoML (supports both ML and deep learning)\nAutoGluon (from Amazon, designed for ease and flexibility)"
  },
  {
    "objectID": "qslides.html#automl-3",
    "href": "qslides.html#automl-3",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nIn this course, we’ll use AutoGluon, a powerful AutoML framework developed by Amazon.\nAutoGluon is:\n\nEasy to use with minimal code\nWell-suited for tabular data\nCapable of trying many models and combining them automatically\nSurprisingly accurate even without manual tuning\n\nIt’s a great tool for quickly building high-quality models — and ideal for learners who want to focus on insights rather than low-level implementation."
  },
  {
    "objectID": "qslides.html#automl-4",
    "href": "qslides.html#automl-4",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nPrediction with AutoGluon begins with the following line of code:\n\npredictor = TabularPredictor(\"Survived\")\n\nWhat does TabularPredictor mean?\nThe term “tabular” refers to data organized in rows and columns — like spreadsheets or pandas DataFrames. This format is typical for structured datasets such as Titanic passenger records.\nUnlike traditional approaches where a single model (e.g., decision tree) is manually selected, AutoGluon automatically trains and compares a variety of powerful algorithms — such as gradient boosting, random forests, and neural networks. It then selects or combines the top-performing ones to generate robust predictions.\nIn short, TabularPredictor manages the entire process for tabular datasets — from training multiple models to picking the most effective strategy — with minimal code required."
  },
  {
    "objectID": "qslides.html#automl-5",
    "href": "qslides.html#automl-5",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nThe next step is to train the model using AutoGluon:\n\npredictor.fit(df_train)\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x758670475cd0&gt;\n\n\nWhat makes this step powerful is its simplicity.\nYou don’t need to perform any manual preprocessing — AutoGluon automatically detects categorical variables and applies one-hot encoding or other suitable transformations for you.\nMoreover, you don’t even need to drop potentially problematic columns like “Name” or “Ticket” — AutoGluon handles them gracefully without breaking the model. It’s incredibly convenient.\nRunning this single line of code is essentially equivalent to performing multiple rounds of model.fit(...) using different algorithms, hyperparameter settings, and preprocessing strategies — all done automatically.\nIt’s like launching an entire machine learning pipeline with just one command."
  },
  {
    "objectID": "qslides.html#automl-6",
    "href": "qslides.html#automl-6",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nLet’s take a look at the model leaderboard results:\n\npredictor.leaderboard()\n\n\n\n\n\n\n\n\nmodel\nscore_val\neval_metric\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n0.839161\naccuracy\n0.002208\n0.388233\n0.002208\n0.388233\n1\nTrue\n3\n\n\n1\nWeightedEnsemble_L2\n0.839161\naccuracy\n0.002688\n0.446935\n0.000481\n0.058703\n2\nTrue\n14\n\n\n2\nLightGBM\n0.832168\naccuracy\n0.002013\n0.383913\n0.002013\n0.383913\n1\nTrue\n4\n\n\n3\nNeuralNetTorch\n0.832168\naccuracy\n0.007735\n1.416502\n0.007735\n1.416502\n1\nTrue\n12\n\n\n4\nXGBoost\n0.825175\naccuracy\n0.003473\n0.158237\n0.003473\n0.158237\n1\nTrue\n11\n\n\n5\nCatBoost\n0.818182\naccuracy\n0.002316\n0.371811\n0.002316\n0.371811\n1\nTrue\n7\n\n\n6\nLightGBMLarge\n0.818182\naccuracy\n0.006671\n0.654833\n0.006671\n0.654833\n1\nTrue\n13\n\n\n7\nExtraTreesEntr\n0.818182\naccuracy\n0.045840\n0.632927\n0.045840\n0.632927\n1\nTrue\n9\n\n\n8\nNeuralNetFastAI\n0.811189\naccuracy\n0.005480\n0.348701\n0.005480\n0.348701\n1\nTrue\n10\n\n\n9\nExtraTreesGini\n0.804196\naccuracy\n0.045844\n0.381226\n0.045844\n0.381226\n1\nTrue\n8\n\n\n10\nRandomForestGini\n0.797203\naccuracy\n0.045467\n0.377950\n0.045467\n0.377950\n1\nTrue\n5\n\n\n11\nRandomForestEntr\n0.797203\naccuracy\n0.046375\n0.528388\n0.046375\n0.528388\n1\nTrue\n6\n\n\n12\nKNeighborsDist\n0.594406\naccuracy\n0.001545\n0.007792\n0.001545\n0.007792\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.587413\naccuracy\n0.001186\n0.007807\n0.001186\n0.007807\n1\nTrue\n1"
  },
  {
    "objectID": "qslides.html#automl-7",
    "href": "qslides.html#automl-7",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nAt the top of the leaderboard, we see models like:\n\nLightGBMXT, LightGBM, XGBoost, CatBoost, LightGBMLarge, ExtraTreesEntr, ExtraTreesGini, RandomForestEntr, RandomForestGini\n\nThese are all tree-based models — an evolution of decision tree algorithms. They combine multiple decision trees to improve prediction performance and reduce overfitting. In the domain of tabular data, tree-based models (especially gradient boosting methods) are widely regarded as the state-of-the-art (SOTA). They:\n\nHandle missing values and mixed data types well\nCapture non-linear patterns without requiring complex preprocessing\nOften outperform deep learning models on structured datasets\n\nThat’s why we consistently see tree-based models dominating AutoML leaderboards — they are fast, accurate, and robust for most practical applications in tabular data analysis."
  },
  {
    "objectID": "qslides.html#automl-8",
    "href": "qslides.html#automl-8",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nNow let’s check the final prediction result:\n\n(y_test == predictor.predict(df_test)).mean()\n\n0.804469\n\n\nEven though we didn’t perform any manual preprocessing on the variables, AutoGluon ran without errors and produced a respectable accuracy of 0.804469.\nThat’s a strong result — especially considering the minimal effort required."
  },
  {
    "objectID": "qslides.html#automl-9",
    "href": "qslides.html#automl-9",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nThis time, we apply a bit of preprocessing by removing a few variables. We simply exclude PassengerId, Ticket, and Cabin, and retrain the model.\n\ndf_train_preprocessed = df_train.drop(['PassengerId','Ticket','Cabin'],axis=1)\ndf_test_preprocessed = df_test.drop(['PassengerId','Ticket','Cabin'],axis=1)\npredictor = TabularPredictor(\"Survived\")\npredictor.fit(df_train_preprocessed)\nyhat_test = predictor.predict(df_test_preprocessed)\n(y_test == yhat_test).mean()\n\n0.810056\n\n\nThe result is 0.810056. This is the best performance among all the models we’ve used so far. Even with the removal of just a few columns, we observe a noticeable improvement in predictive performance."
  },
  {
    "objectID": "qslides.html#automl-10",
    "href": "qslides.html#automl-10",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nHas anyone here wondered something like this?\n\n“Why didn’t we drop the ‘Name’ column? Was it just a mistake?”\n\nActually, it wasn’t a mistake at all.\nDo you remember the final scene of the movie Titanic? When Rose is asked for her name, she replies with a different last name. That moment isn’t just about identity—it’s a statement of her choices and transformation. In the same way, a person’s name in data isn’t just a label. It often carries hidden signals about age, gender, and social status."
  },
  {
    "objectID": "qslides.html#automl-11",
    "href": "qslides.html#automl-11",
    "title": "AI 기반 데이터 사이언스",
    "section": "AutoML",
    "text": "AutoML\nAutoGluon understands this. It doesn’t treat the Name column as a meaningless string. Instead, without any manual feature engineering, it applies simple natural language processing techniques under the hood to extract valuable features.\nFor example, look at the following:\n\ndf_test[:5]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n709\n710\n3\nMoubarek, Master. Halim Gonios (\"William George\")\nmale\nNaN\n1\n1\n2661\n15.2458\nNaN\nC\n\n\n439\n440\n2\nKvillner, Mr. Johan Henrik Johannesson\nmale\n31.0\n0\n0\nC.A. 18723\n10.5000\nNaN\nS\n\n\n840\n841\n3\nAlhomaki, Mr. Ilmari Rudolf\nmale\n20.0\n0\n0\nSOTON/O2 3101287\n7.9250\nNaN\nS\n\n\n720\n721\n2\nHarper, Miss. Annie Jessie \"Nina\"\nfemale\n6.0\n0\n1\n248727\n33.0000\nNaN\nS\n\n\n39\n40\n3\nNicola-Yarred, Miss. Jamila\nfemale\n14.0\n1\n0\n2651\n11.2417\nNaN\nC\n\n\n\n\n\n\n\nWhen I saw the first observation, I predicted that the passenger would survive. Why? Because their name contained the word “Master”, which usually refers to young boys—typically under the age of 10. And as we know, young children had a much higher chance of survival."
  },
  {
    "objectID": "qslides.html#질의응답-실습",
    "href": "qslides.html#질의응답-실습",
    "title": "AI 기반 데이터 사이언스",
    "section": "질의응답 & 실습",
    "text": "질의응답 & 실습"
  }
]